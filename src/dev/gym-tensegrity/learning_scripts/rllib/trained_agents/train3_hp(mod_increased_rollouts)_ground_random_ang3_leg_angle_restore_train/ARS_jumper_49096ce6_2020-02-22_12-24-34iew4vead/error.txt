Failure # 1 (occurred at 2020-03-01_14-02-27)
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py", line 426, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py", line 378, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/usr/local/lib/python3.6/dist-packages/ray/worker.py", line 1457, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(NotImplementedError): [36mray::ARSTrainer.train()[39m (pid=50266, ip=172.18.0.6)
  File "/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py", line 176, in train
    result = self._train()
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/ars/ars.py", line 211, in _train
    theta_id, config["num_rollouts"])
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/ars/ars.py", line 316, in _collect_results
    for result in ray_get_and_free(rollout_ids):
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/memory.py", line 33, in ray_get_and_free
    result = ray.get(object_ids)
ray.exceptions.RayTaskError(RayOutOfMemoryError): [36mray::Worker[39m (pid=50218, ip=172.18.0.6)
  File "python/ray/_raylet.pyx", line 627, in ray._raylet.execute_task
  File "/usr/local/lib/python3.6/dist-packages/ray/memory_monitor.py", line 130, in raise_if_low_memory
    self.error_threshold))
ray.memory_monitor.RayOutOfMemoryError: More than 95% of the memory on node c221379d9cf3 is used (248.5 / 251.37 GB). The top 10 memory consumers are:

PID	MEM	COMMAND
50063	0.32GiB	Xvfb :99 -screen 0 640x480x16 -nolisten tcp -auth /tmp/xvfb-run.5qH1tv/Xauthority
50266	0.27GiB	ray::ARSTrainer.train()
75730	0.27GiB	Xvfb :100 -screen 0 640x480x16 -nolisten tcp -auth /tmp/xvfb-run.iR9HjO/Xauthority
50261	0.25GiB	ray::Worker
50218	0.25GiB	ray::Worker
50240	0.25GiB	ray::Worker
50238	0.25GiB	ray::Worker
50208	0.25GiB	ray::Worker
50260	0.25GiB	ray::Worker
50217	0.25GiB	ray::Worker

In addition, up to 1.05 GiB of shared memory is currently being used by the Ray object store. You can set the object store size with the `object_store_memory` parameter when starting Ray, and the max Redis size with `redis_max_memory`. Note that Ray assumes all system memory is available for use by workers. If your system has other applications running, you should manually set these memory limits to a lower value.

During handling of the above exception, another exception occurred:

[36mray::ARSTrainer.train()[39m (pid=50266, ip=172.18.0.6)
  File "python/ray/_raylet.pyx", line 633, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 634, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 636, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 619, in ray._raylet.execute_task.function_executor
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 438, in train
    self._try_recover()
  File "/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py", line 799, in _try_recover
    "Recovery is not supported for this algorithm")
NotImplementedError: Recovery is not supported for this algorithm

